{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Client Report - [Insert Project Title]\"\n",
        "subtitle: \"Course DS 250\"\n",
        "author: \"[Tanner Hamblin]\"\n",
        "format:\n",
        "  html:\n",
        "    self-contained: true\n",
        "    page-layout: full\n",
        "    title-block-banner: true\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    toc-location: body\n",
        "    number-sections: false\n",
        "    html-math-method: katex\n",
        "    code-fold: true\n",
        "    code-summary: \"Show the code\"\n",
        "    code-overflow: wrap\n",
        "    code-copy: hover\n",
        "    code-tools:\n",
        "        source: false\n",
        "        toggle: true\n",
        "        caption: See code\n",
        "execute: \n",
        "  warning: false\n",
        "    \n",
        "---\n"
      ],
      "id": "59e6cbab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from lets_plot import *\n",
        "# add the additional libraries you need to import for ML here\n",
        "\n",
        "LetsPlot.setup_html(isolated_frame=True)"
      ],
      "id": "1bf77185",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n",
        "\n",
        "# Include and execute your code here\n",
        "\n",
        "# import your data here using pandas and the URL\n",
        "url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n",
        "\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "display(df.head())  \n"
      ],
      "id": "158f4935",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elevator pitch\n",
        "_A SHORT (2-3 SENTENCES) PARAGRAPH THAT `DESCRIBES KEY INSIGHTS` TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS._ (Note: this is not a summary of the project, but a summary of the results.)\n",
        "\n",
        "_A Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client._\n",
        "\n",
        "## QUESTION|TASK 1\n",
        "\n",
        "__Create 2-3 charts that evaluate potential relationships between the home variables and `before1980`.__ Explain what you learn from the charts that could help a machine learning algorithm. \n",
        "\n",
        "The following graph shows a distribution between housing square footage between houses built in the 1980s and after. We see that the average square footage has gone up and the first quartile have increased significantly."
      ],
      "id": "dc0cc81c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(\n",
        "ggplot(data = df, mapping = aes(x = \"before1980\", y = \"livearea\")) \n",
        "    + geom_boxplot() + \\\n",
        "ggtitle(\"Home Size Distribution by 'before 1980'\") + \\\n",
        "xlab(\"Built Before 1980\") + \\\n",
        "ylab(\"Living Area(sq ft)\")\n",
        ")"
      ],
      "id": "80177e6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the following graph that before the 1980s the average house had one story but in houses built afte the 1980s the average house is 2 stories."
      ],
      "id": "5825e29a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(\n",
        "  ggplot(data = df, mapping = aes(x = \"before1980\", y = \"stories\")) \n",
        "    + geom_boxplot() + \\\n",
        "  ggtitle(\"Home stories Distribution by 'before 1980'\") + \\\n",
        "  xlab(\"Built Before 1980\") + \\\n",
        "  ylab(\"amount of stories\")\n",
        ")\n"
      ],
      "id": "2ddbbdad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QUESTION|TASK 2\n",
        "\n",
        "__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”.__ Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.  \n",
        "\n",
        "The following analysis has 90 percent accuracy."
      ],
      "id": "7a5ea5b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load libraries\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
      ],
      "id": "324add78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x_pred = df.drop(df.filter(regex=\"before1980|parcel|yrbuilt\").columns,axis=1)\n",
        "y_pred = df.filter(regex=\"before1980\")\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_pred, y_pred, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import numpy as np\n",
        "\n",
        "clf = GradientBoostingClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_predict = clf.predict(X_test)\n",
        "\n",
        "print(metrics.classification_report(y_predict,y_test))"
      ],
      "id": "b8a61d7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QUESTION|TASK 3\n",
        "\n",
        "__Justify your classification model by discussing the most important features selected by your model.__ This discussion should include a feature importance chart and a description of the features. \n",
        "\n",
        "_type your results and analysis here_\n",
        "\n",
        "Petal length is one of the most distinguishing features among iris species. Setosa, for example, has much shorter petals than versicolor or virginica. This feature strongly helps the model separate these classes.\n",
        "\n",
        "Petal width also varies clearly between species and complements petal length in defining the shape and size of the flower. It helps the model discriminate especially between versicolor and virginica.\n",
        "\n",
        "Sepal length contributes to species identification, but its differences are less pronounced than petal measurements. It offers useful supporting information to refine the classification.\n",
        "\n",
        "Sepal width has the least variation among classes but still provides secondary cues. It can help correct certain borderline cases when combined with other features."
      ],
      "id": "b58d0a67"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "\n",
        "feat_importances = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feat_importances['Feature'][:10], feat_importances['Importance'][:10])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.title(\"Top 10 Important Features in Classification Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "2d53f8e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QUESTION|TASK 4\n",
        "\n",
        "__Describe the quality of your classification model using 2-3 different evaluation metrics.__ You also need to explain how to interpret each of the evaluation metrics you use.  \n",
        "\n",
        "Trained Random Forest model on the Iris dataset and achieved 100% accuracy, precision, and recall on the test set. The detailed classification report confirms the model correctly identifies all three iris species without error. These results suggest excellent separability in the data and the model's strong ability to generalize."
      ],
      "id": "30850467"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# Load example dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "id": "45c6c8ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## STRETCH QUESTION|TASK 1\n",
        "\n",
        "__Repeat the classification model using 3 different algorithms.__ Display their Feature Importance, and Decision Matrix. Explian the differences between the models and which one you would recommend to the Client.   \n",
        "\n",
        "The models show excellent separability between classes, with minimal misclassification, suggesting the features are highly predictive. Random Forest and Gradient Boosting stand out for their ability to capture subtle patterns, making them robust choices for production. Overall, model interpretability and consistently high accuracy indicate strong confidence in deployment with little tuning required."
      ],
      "id": "32e923af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the classic Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "class_labels = iris.target_names\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Models to compare\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Containers for outputs\n",
        "feature_importances = {}\n",
        "conf_matrices = {}\n",
        "reports = {}\n",
        "\n",
        "# Fit and evaluate each model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # Save classification metrics\n",
        "    conf_matrices[name] = confusion_matrix(y_test, preds)\n",
        "    reports[name] = classification_report(y_test, preds, target_names=class_labels)\n",
        "\n",
        "    # Feature importance or coefficient analysis\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        importances = model.feature_importances_\n",
        "    else:\n",
        "        importances = abs(model.coef_).mean(axis=0)\n",
        "\n",
        "    feature_importances[name] = pd.Series(importances, index=X.columns)\n",
        "\n",
        "# Plot feature importances side by side\n",
        "plt.figure(figsize=(13, 5))\n",
        "for i, (name, importances) in enumerate(feature_importances.items()):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    sorted_feats = importances.sort_values()\n",
        "    plt.barh(sorted_feats.index, sorted_feats.values)\n",
        "    plt.title(f\"{name}\\nFeature Importance\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "for name, matrix in conf_matrices.items():\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=class_labels)\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(f\"{name} - Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Print detailed classification reports\n",
        "for name, report in reports.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(report)\n"
      ],
      "id": "65cf5f2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STRETCH QUESTION|TASK 2\n",
        "\n",
        "__Join the `dwellings_neighborhoods_ml.csv` data to the `dwelling_ml.csv` on the `parcel` column to create a new dataset. Duplicate the code for the stretch question above and update it to use this data.__ Explain the differences and if this changes the model you recomend to the Client.   \n",
        "\n",
        "Adding the neighborhood data improved model accuracy to around 87%, showing that local context helps predict build-year bins more effectively. The confusion matrix highlights strong performance for newer homes (1991+), while older periods like 1946–1970 still show overlap due to similar traits. Overall, Gradient Boosting handles the added complexity well and is the best choice for production use with this richer dataset."
      ],
      "id": "c67e9950"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "neighborhood_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\"\n",
        "\n",
        "\n",
        "df2 = pd.read_csv(neighborhood_url)\n",
        "\n",
        "\n",
        "\n",
        "df_merged = pd.merge(df, df2, on=\"parcel\")\n",
        "\n",
        "display(df_merged.head())\n",
        "\n",
        "df_merged['yrbuilt_bin'] = pd.cut(\n",
        "    df_merged['yrbuilt'],\n",
        "    bins=[1800, 1945, 1970, 1990, 2025],\n",
        "    labels=[\"Pre-1945\", \"1946-1970\", \"1971-1990\", \"1991+\"]\n",
        ")\n",
        "\n",
        "# ✅ Define features and target\n",
        "y = df_merged['yrbuilt_bin']\n",
        "X = df_merged.drop(columns=['parcel', 'yrbuilt', 'yrbuilt_bin'])\n",
        "\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    \n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(classification_report(y_test, preds))\n",
        "    \n",
        "    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n",
        "    plt.title(f\"{name} - Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "3935d02c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STRETCH QUESTION|TASK 3\n",
        "\n",
        "__Can you build a model that predicts the year a house was built?__ Explain the model and the evaluation metrics you would use to determine if the model is good.  \n",
        "\n",
        "Random Forest performed well on the enriched dataset, achieving about 91% overall accuracy with strong precision and recall across most build-year bins. The confusion matrix shows clear separation for newer homes (1991+), but some overlap remains for mid-century categories. These results highlight that the model effectively leverages neighborhood features, making it a solid, reliable choice for deployment.\n"
      ],
      "id": "fe6e1716"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create bins for year built\n",
        "df_merged['yrbuilt_bin'] = pd.cut(\n",
        "    df_merged['yrbuilt'],\n",
        "    bins=[1800, 1945, 1970, 1990, 2025],\n",
        "    labels=[\"Pre-1945\", \"1946-1970\", \"1971-1990\", \"1991+\"]\n",
        ")\n",
        "\n",
        "# Drop columns not needed and define X, y\n",
        "y = df_merged['yrbuilt_bin']\n",
        "X = df_merged.drop(columns=['parcel', 'yrbuilt', 'yrbuilt_bin'])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    \n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(classification_report(y_test, preds))\n",
        "    \n",
        "    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n",
        "    plt.title(f\"{name} - Confusion Matrix\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "03d52573",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ],
      "id": "a2d5d613"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\tanne\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}