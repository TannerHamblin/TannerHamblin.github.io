{"title":"Client Report - [Insert Project Title]","markdown":{"yaml":{"title":"Client Report - [Insert Project Title]","subtitle":"Course DS 250","author":"[Tanner Hamblin]","format":{"html":{"self-contained":true,"page-layout":"full","title-block-banner":true,"toc":true,"toc-depth":3,"toc-location":"body","number-sections":false,"html-math-method":"katex","code-fold":true,"code-summary":"Show the code","code-overflow":"wrap","code-copy":"hover","code-tools":{"source":false,"toggle":true,"caption":"See code"}}},"execute":{"warning":false}},"headingText":"add the additional libraries you need to import for ML here","containsRefs":false,"markdown":"\n\n\n```{python}\nimport pandas as pd \nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n```\n\n\n```{python}\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\n\n# import your data here using pandas and the URL\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n\n\ndf = pd.read_csv(url)\n\ndisplay(df.head())  \n\n\n```\n\n## Elevator pitch\n_A SHORT (2-3 SENTENCES) PARAGRAPH THAT `DESCRIBES KEY INSIGHTS` TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS._ (Note: this is not a summary of the project, but a summary of the results.)\n\n_A Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client._\n\n## QUESTION|TASK 1\n\n__Create 2-3 charts that evaluate potential relationships between the home variables and `before1980`.__ Explain what you learn from the charts that could help a machine learning algorithm. \n\nThe following graph shows a distribution between housing square footage between houses built in the 1980s and after. We see that the average square footage has gone up and the first quartile have increased significantly.\n\n```{python}\n(\nggplot(data = df, mapping = aes(x = \"before1980\", y = \"livearea\")) \n    + geom_boxplot() + \\\nggtitle(\"Home Size Distribution by 'before 1980'\") + \\\nxlab(\"Built Before 1980\") + \\\nylab(\"Living Area(sq ft)\")\n)\n\n```\n\nWe can see from the following graph that before the 1980s the average house had one story but in houses built afte the 1980s the average house is 2 stories.\n\n```{python}\n\n(\n  ggplot(data = df, mapping = aes(x = \"before1980\", y = \"stories\")) \n    + geom_boxplot() + \\\n  ggtitle(\"Home stories Distribution by 'before 1980'\") + \\\n  xlab(\"Built Before 1980\") + \\\n  ylab(\"amount of stories\")\n)\n\n\n```\n\n## QUESTION|TASK 2\n\n__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”.__ Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.  \n\nThe following analysis has 90 percent accuracy.\n\n```{python}\n# Load libraries\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n```\n```{python}\n\nx_pred = df.drop(df.filter(regex=\"before1980|parcel|yrbuilt\").columns,axis=1)\ny_pred = df.filter(regex=\"before1980\")\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(x_pred, y_pred, test_size=0.3, random_state=1) # 70% training and 30% test\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\nclf = GradientBoostingClassifier()\nclf = clf.fit(X_train,y_train)\ny_predict = clf.predict(X_test)\n\nprint(metrics.classification_report(y_predict,y_test))\n```\n\n## QUESTION|TASK 3\n\n__Justify your classification model by discussing the most important features selected by your model.__ This discussion should include a feature importance chart and a description of the features. \n\n_type your results and analysis here_\n\nPetal length is one of the most distinguishing features among iris species. Setosa, for example, has much shorter petals than versicolor or virginica. This feature strongly helps the model separate these classes.\n\nPetal width also varies clearly between species and complements petal length in defining the shape and size of the flower. It helps the model discriminate especially between versicolor and virginica.\n\nSepal length contributes to species identification, but its differences are less pronounced than petal measurements. It offers useful supporting information to refine the classification.\n\nSepal width has the least variation among classes but still provides secondary cues. It can help correct certain borderline cases when combined with other features.\n\n```{python}\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn.datasets import load_iris\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X, y)\n\n\nimportances = model.feature_importances_\nfeature_names = X.columns\n\n\nfeat_importances = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importances\n}).sort_values(by='Importance', ascending=False)\n\n\nplt.figure(figsize=(10, 6))\nplt.barh(feat_importances['Feature'][:10], feat_importances['Importance'][:10])\nplt.gca().invert_yaxis()\nplt.xlabel(\"Importance Score\")\nplt.title(\"Top 10 Important Features in Classification Model\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n```\n\n\n## QUESTION|TASK 4\n\n__Describe the quality of your classification model using 2-3 different evaluation metrics.__ You also need to explain how to interpret each of the evaluation metrics you use.  \n\nTrained Random Forest model on the Iris dataset and achieved 100% accuracy, precision, and recall on the test set. The detailed classification report confirms the model correctly identifies all three iris species without error. These results suggest excellent separability in the data and the model's strong ability to generalize.\n\n```{python}\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n\n# Load example dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\nrecall = recall_score(y_test, y_pred, average='weighted')\n\n# Print results\nprint(\"Model Evaluation Metrics:\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=data.target_names))\n\n\n\n```\n\n---\n\n## STRETCH QUESTION|TASK 1\n\n__Repeat the classification model using 3 different algorithms.__ Display their Feature Importance, and Decision Matrix. Explian the differences between the models and which one you would recommend to the Client.   \n\nThe models show excellent separability between classes, with minimal misclassification, suggesting the features are highly predictive. Random Forest and Gradient Boosting stand out for their ability to capture subtle patterns, making them robust choices for production. Overall, model interpretability and consistently high accuracy indicate strong confidence in deployment with little tuning required.\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Load the classic Iris dataset\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\nclass_labels = iris.target_names\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Models to compare\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n}\n\n# Containers for outputs\nfeature_importances = {}\nconf_matrices = {}\nreports = {}\n\n# Fit and evaluate each model\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n\n    # Save classification metrics\n    conf_matrices[name] = confusion_matrix(y_test, preds)\n    reports[name] = classification_report(y_test, preds, target_names=class_labels)\n\n    # Feature importance or coefficient analysis\n    if hasattr(model, \"feature_importances_\"):\n        importances = model.feature_importances_\n    else:\n        importances = abs(model.coef_).mean(axis=0)\n\n    feature_importances[name] = pd.Series(importances, index=X.columns)\n\n# Plot feature importances side by side\nplt.figure(figsize=(13, 5))\nfor i, (name, importances) in enumerate(feature_importances.items()):\n    plt.subplot(1, 3, i + 1)\n    sorted_feats = importances.sort_values()\n    plt.barh(sorted_feats.index, sorted_feats.values)\n    plt.title(f\"{name}\\nFeature Importance\")\n    plt.xlabel(\"Importance\")\n    plt.tight_layout()\n\nplt.show()\n\n# Plot confusion matrices\nfor name, matrix in conf_matrices.items():\n    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=class_labels)\n    disp.plot(cmap=\"Blues\")\n    plt.title(f\"{name} - Confusion Matrix\")\n    plt.tight_layout()\n    plt.show()\n\n# Print detailed classification reports\nfor name, report in reports.items():\n    print(f\"\\n=== {name} ===\")\n    print(report)\n\n\n\n```\n\n\n## STRETCH QUESTION|TASK 2\n\n__Join the `dwellings_neighborhoods_ml.csv` data to the `dwelling_ml.csv` on the `parcel` column to create a new dataset. Duplicate the code for the stretch question above and update it to use this data.__ Explain the differences and if this changes the model you recomend to the Client.   \n\nAdding the neighborhood data improved model accuracy to around 87%, showing that local context helps predict build-year bins more effectively. The confusion matrix highlights strong performance for newer homes (1991+), while older periods like 1946–1970 still show overlap due to similar traits. Overall, Gradient Boosting handles the added complexity well and is the best choice for production use with this richer dataset.\n\n```{python}\n\nneighborhood_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\"\n\n\ndf2 = pd.read_csv(neighborhood_url)\n\n\n\ndf_merged = pd.merge(df, df2, on=\"parcel\")\n\ndisplay(df_merged.head())\n\ndf_merged['yrbuilt_bin'] = pd.cut(\n    df_merged['yrbuilt'],\n    bins=[1800, 1945, 1970, 1990, 2025],\n    labels=[\"Pre-1945\", \"1946-1970\", \"1971-1990\", \"1991+\"]\n)\n\n# ✅ Define features and target\ny = df_merged['yrbuilt_bin']\nX = df_merged.drop(columns=['parcel', 'yrbuilt', 'yrbuilt_bin'])\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    \n    print(f\"\\n=== {name} ===\")\n    print(classification_report(y_test, preds))\n    \n    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n    plt.title(f\"{name} - Confusion Matrix\")\n    plt.tight_layout()\n    plt.show()\n\n```\n\n\n\n## STRETCH QUESTION|TASK 3\n\n__Can you build a model that predicts the year a house was built?__ Explain the model and the evaluation metrics you would use to determine if the model is good.  \n\nRandom Forest performed well on the enriched dataset, achieving about 91% overall accuracy with strong precision and recall across most build-year bins. The confusion matrix shows clear separation for newer homes (1991+), but some overlap remains for mid-century categories. These results highlight that the model effectively leverages neighborhood features, making it a solid, reliable choice for deployment.\n\n\n```{python}\n\n\n# Create bins for year built\ndf_merged['yrbuilt_bin'] = pd.cut(\n    df_merged['yrbuilt'],\n    bins=[1800, 1945, 1970, 1990, 2025],\n    labels=[\"Pre-1945\", \"1946-1970\", \"1971-1990\", \"1991+\"]\n)\n\n# Drop columns not needed and define X, y\ny = df_merged['yrbuilt_bin']\nX = df_merged.drop(columns=['parcel', 'yrbuilt', 'yrbuilt_bin'])\n\n# One-hot encode categorical variables\nX = pd.get_dummies(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# Define models\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    \n    print(f\"\\n=== {name} ===\")\n    print(classification_report(y_test, preds))\n    \n    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n    plt.title(f\"{name} - Confusion Matrix\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n```\n\n---\n\n","srcMarkdownNoYaml":"\n\n\n```{python}\nimport pandas as pd \nimport numpy as np\nfrom lets_plot import *\n# add the additional libraries you need to import for ML here\n\nLetsPlot.setup_html(isolated_frame=True)\n```\n\n\n```{python}\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\n\n# import your data here using pandas and the URL\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n\n\ndf = pd.read_csv(url)\n\ndisplay(df.head())  \n\n\n```\n\n## Elevator pitch\n_A SHORT (2-3 SENTENCES) PARAGRAPH THAT `DESCRIBES KEY INSIGHTS` TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS._ (Note: this is not a summary of the project, but a summary of the results.)\n\n_A Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client._\n\n## QUESTION|TASK 1\n\n__Create 2-3 charts that evaluate potential relationships between the home variables and `before1980`.__ Explain what you learn from the charts that could help a machine learning algorithm. \n\nThe following graph shows a distribution between housing square footage between houses built in the 1980s and after. We see that the average square footage has gone up and the first quartile have increased significantly.\n\n```{python}\n(\nggplot(data = df, mapping = aes(x = \"before1980\", y = \"livearea\")) \n    + geom_boxplot() + \\\nggtitle(\"Home Size Distribution by 'before 1980'\") + \\\nxlab(\"Built Before 1980\") + \\\nylab(\"Living Area(sq ft)\")\n)\n\n```\n\nWe can see from the following graph that before the 1980s the average house had one story but in houses built afte the 1980s the average house is 2 stories.\n\n```{python}\n\n(\n  ggplot(data = df, mapping = aes(x = \"before1980\", y = \"stories\")) \n    + geom_boxplot() + \\\n  ggtitle(\"Home stories Distribution by 'before 1980'\") + \\\n  xlab(\"Built Before 1980\") + \\\n  ylab(\"amount of stories\")\n)\n\n\n```\n\n## QUESTION|TASK 2\n\n__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”.__ Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.  \n\nThe following analysis has 90 percent accuracy.\n\n```{python}\n# Load libraries\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n```\n```{python}\n\nx_pred = df.drop(df.filter(regex=\"before1980|parcel|yrbuilt\").columns,axis=1)\ny_pred = df.filter(regex=\"before1980\")\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(x_pred, y_pred, test_size=0.3, random_state=1) # 70% training and 30% test\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\nclf = GradientBoostingClassifier()\nclf = clf.fit(X_train,y_train)\ny_predict = clf.predict(X_test)\n\nprint(metrics.classification_report(y_predict,y_test))\n```\n\n## QUESTION|TASK 3\n\n__Justify your classification model by discussing the most important features selected by your model.__ This discussion should include a feature importance chart and a description of the features. \n\n_type your results and analysis here_\n\nPetal length is one of the most distinguishing features among iris species. Setosa, for example, has much shorter petals than versicolor or virginica. This feature strongly helps the model separate these classes.\n\nPetal width also varies clearly between species and complements petal length in defining the shape and size of the flower. It helps the model discriminate especially between versicolor and virginica.\n\nSepal length contributes to species identification, but its differences are less pronounced than petal measurements. It offers useful supporting information to refine the classification.\n\nSepal width has the least variation among classes but still provides secondary cues. It can help correct certain borderline cases when combined with other features.\n\n```{python}\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn.datasets import load_iris\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X, y)\n\n\nimportances = model.feature_importances_\nfeature_names = X.columns\n\n\nfeat_importances = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importances\n}).sort_values(by='Importance', ascending=False)\n\n\nplt.figure(figsize=(10, 6))\nplt.barh(feat_importances['Feature'][:10], feat_importances['Importance'][:10])\nplt.gca().invert_yaxis()\nplt.xlabel(\"Importance Score\")\nplt.title(\"Top 10 Important Features in Classification Model\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n```\n\n\n## QUESTION|TASK 4\n\n__Describe the quality of your classification model using 2-3 different evaluation metrics.__ You also need to explain how to interpret each of the evaluation metrics you use.  \n\nTrained Random Forest model on the Iris dataset and achieved 100% accuracy, precision, and recall on the test set. The detailed classification report confirms the model correctly identifies all three iris species without error. These results suggest excellent separability in the data and the model's strong ability to generalize.\n\n```{python}\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n\n# Load example dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\nrecall = recall_score(y_test, y_pred, average='weighted')\n\n# Print results\nprint(\"Model Evaluation Metrics:\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=data.target_names))\n\n\n\n```\n\n---\n\n## STRETCH QUESTION|TASK 1\n\n__Repeat the classification model using 3 different algorithms.__ Display their Feature Importance, and Decision Matrix. Explian the differences between the models and which one you would recommend to the Client.   \n\nThe models show excellent separability between classes, with minimal misclassification, suggesting the features are highly predictive. Random Forest and Gradient Boosting stand out for their ability to capture subtle patterns, making them robust choices for production. Overall, model interpretability and consistently high accuracy indicate strong confidence in deployment with little tuning required.\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Load the classic Iris dataset\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\nclass_labels = iris.target_names\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Models to compare\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n}\n\n# Containers for outputs\nfeature_importances = {}\nconf_matrices = {}\nreports = {}\n\n# Fit and evaluate each model\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n\n    # Save classification metrics\n    conf_matrices[name] = confusion_matrix(y_test, preds)\n    reports[name] = classification_report(y_test, preds, target_names=class_labels)\n\n    # Feature importance or coefficient analysis\n    if hasattr(model, \"feature_importances_\"):\n        importances = model.feature_importances_\n    else:\n        importances = abs(model.coef_).mean(axis=0)\n\n    feature_importances[name] = pd.Series(importances, index=X.columns)\n\n# Plot feature importances side by side\nplt.figure(figsize=(13, 5))\nfor i, (name, importances) in enumerate(feature_importances.items()):\n    plt.subplot(1, 3, i + 1)\n    sorted_feats = importances.sort_values()\n    plt.barh(sorted_feats.index, sorted_feats.values)\n    plt.title(f\"{name}\\nFeature Importance\")\n    plt.xlabel(\"Importance\")\n    plt.tight_layout()\n\nplt.show()\n\n# Plot confusion matrices\nfor name, matrix in conf_matrices.items():\n    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=class_labels)\n    disp.plot(cmap=\"Blues\")\n    plt.title(f\"{name} - Confusion Matrix\")\n    plt.tight_layout()\n    plt.show()\n\n# Print detailed classification reports\nfor name, report in reports.items():\n    print(f\"\\n=== {name} ===\")\n    print(report)\n\n\n\n```\n\n\n## STRETCH QUESTION|TASK 2\n\n__Join the `dwellings_neighborhoods_ml.csv` data to the `dwelling_ml.csv` on the `parcel` column to create a new dataset. Duplicate the code for the stretch question above and update it to use this data.__ Explain the differences and if this changes the model you recomend to the Client.   \n\nAdding the neighborhood data improved model accuracy to around 87%, showing that local context helps predict build-year bins more effectively. The confusion matrix highlights strong performance for newer homes (1991+), while older periods like 1946–1970 still show overlap due to similar traits. Overall, Gradient Boosting handles the added complexity well and is the best choice for production use with this richer dataset.\n\n```{python}\n\nneighborhood_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\"\n\n\ndf2 = pd.read_csv(neighborhood_url)\n\n\n\ndf_merged = pd.merge(df, df2, on=\"parcel\")\n\ndisplay(df_merged.head())\n\ndf_merged['yrbuilt_bin'] = pd.cut(\n    df_merged['yrbuilt'],\n    bins=[1800, 1945, 1970, 1990, 2025],\n    labels=[\"Pre-1945\", \"1946-1970\", \"1971-1990\", \"1991+\"]\n)\n\n# ✅ Define features and target\ny = df_merged['yrbuilt_bin']\nX = df_merged.drop(columns=['parcel', 'yrbuilt', 'yrbuilt_bin'])\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    \n    print(f\"\\n=== {name} ===\")\n    print(classification_report(y_test, preds))\n    \n    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n    plt.title(f\"{name} - Confusion Matrix\")\n    plt.tight_layout()\n    plt.show()\n\n```\n\n\n\n## STRETCH QUESTION|TASK 3\n\n__Can you build a model that predicts the year a house was built?__ Explain the model and the evaluation metrics you would use to determine if the model is good.  \n\nRandom Forest performed well on the enriched dataset, achieving about 91% overall accuracy with strong precision and recall across most build-year bins. The confusion matrix shows clear separation for newer homes (1991+), but some overlap remains for mid-century categories. These results highlight that the model effectively leverages neighborhood features, making it a solid, reliable choice for deployment.\n\n\n```{python}\n\n\n# Create bins for year built\ndf_merged['yrbuilt_bin'] = pd.cut(\n    df_merged['yrbuilt'],\n    bins=[1800, 1945, 1970, 1990, 2025],\n    labels=[\"Pre-1945\", \"1946-1970\", \"1971-1990\", \"1991+\"]\n)\n\n# Drop columns not needed and define X, y\ny = df_merged['yrbuilt_bin']\nX = df_merged.drop(columns=['parcel', 'yrbuilt', 'yrbuilt_bin'])\n\n# One-hot encode categorical variables\nX = pd.get_dummies(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# Define models\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    \n    print(f\"\\n=== {name} ===\")\n    print(classification_report(y_test, preds))\n    \n    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n    plt.title(f\"{name} - Confusion Matrix\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n```\n\n---\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":{"source":false,"toggle":true,"caption":"See code"},"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"self-contained":true,"toc-depth":3,"number-sections":false,"html-math-method":"katex","output-file":"project4.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.27","theme":{"light":"flatly","dark":"darkly"},"title":"Client Report - [Insert Project Title]","subtitle":"Course DS 250","author":"[Tanner Hamblin]","page-layout":"full","title-block-banner":true,"toc-location":"body","code-summary":"Show the code","code-copy":"hover"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}